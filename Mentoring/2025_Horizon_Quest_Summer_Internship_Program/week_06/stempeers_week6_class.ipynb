{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Summer Session: Machine Learning for High School Students**  \n",
        "## **Week 6: Linear Regression - Theory and Implementation**  \n",
        "\n",
        "### **1. Introduction to Linear Regression (30 min)**  \n",
        "**Objective:** Understand the mathematical foundations of linear regression, including different optimization methods.  \n",
        "\n",
        "#### **1.1 What is Linear Regression?**  \n",
        "- A supervised learning algorithm for predicting continuous numerical values.  \n",
        "- **Example Applications:**  \n",
        "  - Predicting house prices based on square footage.  \n",
        "  - Estimating student test scores based on study hours.  \n",
        "\n",
        "#### **1.2 Simple Linear Regression Equation**  \n",
        "The model assumes a linear relationship between input `X` and output `y`:  \n",
        "\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 X + \\epsilon\n",
        "$$  \n",
        "\n",
        "- $y$: Target variable (dependent variable).  \n",
        "- $X$: Feature (independent variable).  \n",
        "- $ \\beta_0 $: Intercept (bias term).  \n",
        "- $ \\beta_1 $: Slope (coefficient).  \n",
        "- $ \\epsilon $: Error term (residuals).  \n",
        "\n",
        "#### **1.3 Cost Function: Mean Squared Error (MSE)**  \n",
        "The goal is to minimize the difference between predicted and actual values:  \n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "- $ n $: Number of data points.  \n",
        "- $ y_i $: Actual value.  \n",
        "- $ \\hat{y}_i $: Predicted value.  \n",
        "\n",
        "#### **1.4 Optimization Methods**  \n",
        "\n",
        "##### **Method 1: Normal Equation (Closed-Form Solution)**  \n",
        "- Directly computes optimal coefficients without iteration.  \n",
        "- Formula:  \n",
        "\n",
        "$$\n",
        "\\beta = (X^T X)^{-1} X^T y\n",
        "$$  \n",
        "\n",
        "**Advantages:**  \n",
        "✔ Exact solution (no approximation).  \n",
        "✔ Works well for small datasets.  \n",
        "\n",
        "**Disadvantages:**  \n",
        "✖ Computationally expensive for large datasets $(O(n^3)$).  \n",
        "✖ Requires matrix inversion (fails if $X^T X$ is singular).  \n",
        "\n",
        "##### **Method 2: Gradient Descent (Iterative Approach)**  \n",
        "- Gradually adjusts coefficients to minimize MSE.  \n",
        "- Update rule:  \n",
        "\n",
        "$$\n",
        "\\beta_j := \\beta_j - \\alpha \\frac{\\partial}{\\partial \\beta_j} MSE\n",
        "$$\n",
        "\n",
        "- $\\alpha$: Learning rate (controls step size).  \n",
        "\n",
        "**Advantages:**  \n",
        "✔ Scalable for large datasets (\\(O(n)\\) per iteration).  \n",
        "✔ Works well with high-dimensional data.  \n",
        "\n",
        "**Disadvantages:**  \n",
        "✖ Requires tuning learning rate.  \n",
        "✖ May converge to local minima (rare in linear regression).  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Implementing Linear Regression in Python (60 min)**  \n",
        "\n",
        "#### **Method 1: Using Scikit-learn**  \n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()  # Uses Normal Equation internally\n",
        "model.fit(X_train, y_train)\n",
        "print(f\"Slope (β₁): {model.coef_[0]}, Intercept (β₀): {model.intercept_}\")\n",
        "```\n",
        "\n",
        "#### **Method 2: Normal Equation from Scratch**  \n",
        "```python\n",
        "def normal_equation(X, y):\n",
        "    X_b = np.c_[np.ones((len(X), 1)), X]  # Add bias term (β₀)\n",
        "    beta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
        "    return beta\n",
        "\n",
        "# Usage\n",
        "X = np.array([1000, 1500, 1200, 1800, 2000])\n",
        "y = np.array([300000, 400000, 350000, 450000, 500000])\n",
        "\n",
        "beta = normal_equation(X.reshape(-1, 1), y)\n",
        "print(f\"Intercept (β₀): {beta[0]}, Slope (β₁): {beta[1]}\")\n",
        "```\n",
        "\n",
        "#### **Method 3: Gradient Descent from Scratch**  \n",
        "```python\n",
        "def gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n",
        "    n = len(X)\n",
        "    beta = np.random.randn(2, 1)  # Random initialization\n",
        "    \n",
        "    for _ in range(epochs):\n",
        "        X_b = np.c_[np.ones((n, 1)), X]  # Add bias term\n",
        "        y_pred = X_b @ beta\n",
        "        gradients = (2/n) * X_b.T @ (y_pred - y)\n",
        "        beta -= learning_rate * gradients\n",
        "    \n",
        "    return beta\n",
        "\n",
        "beta = gradient_descent(X.reshape(-1, 1), y.reshape(-1, 1))\n",
        "print(f\"Intercept (β₀): {beta[0][0]}, Slope (β₁): {beta[1][0]}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Hands-on Exercise (30 min)**  \n",
        "**Task:** Compare Normal Equation and Gradient Descent on the `diabetes` dataset.  \n",
        "\n",
        "1. Load the dataset:  \n",
        "   ```python\n",
        "   from sklearn.datasets import load_diabetes\n",
        "   data = load_diabetes()\n",
        "   X, y = data.data, data.target\n",
        "   ```  \n",
        "2. Implement both methods.  \n",
        "3. Compare runtime and coefficients.  \n",
        "\n",
        "**Discussion Questions:**  \n",
        "- Which method is faster for small datasets?  \n",
        "- What happens if \\(X^T X\\) is non-invertible?  \n",
        "- How does learning rate affect gradient descent?  \n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**  \n",
        "- **Theory:**  \n",
        "  - Linear regression models relationships using coefficients.  \n",
        "  - Normal equation is exact but slow for large data.  \n",
        "  - Gradient descent is scalable but requires tuning.  \n",
        "- **Coding:**  \n",
        "  - Implemented regression using Scikit-learn, normal equation, and gradient descent.  \n",
        "- **Next Week:** Decision Trees for classification!"
      ],
      "metadata": {
        "id": "Yk2PC3HhkELX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o2lKd9BwkFLk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}